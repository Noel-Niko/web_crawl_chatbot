{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "!pip install -r requirements.txt",
   "id": "5bc0704a18b3cbbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# User-Agent header\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    try:\n",
    "        # Modify this line to include the headers parameter\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "    return parser.hyperlinks\n",
    "\n",
    "\n",
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# Define root domain to crawl\n",
    "domain = \"ci.hartford.wi.us\"\n",
    "# Full URL including the specific path to start crawling from\n",
    "full_url = \"https://ci.hartford.wi.us/440/Experience-Downtown\"\n",
    "\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "    \n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        try: \n",
    "            if re.search(HTTP_URL_PATTERN, link):\n",
    "                # Parse the URL and check if the domain is the same\n",
    "                url_obj = urlparse(link)\n",
    "                if url_obj.netloc == local_domain:\n",
    "                    clean_link = link\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                 \n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            try:\n",
    "                if link.startswith(\"/\"):\n",
    "                    link = link[1:]\n",
    "                elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                    continue\n",
    "                clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        if clean_link is not None:\n",
    "            try:\n",
    "                if clean_link.endswith(\"/\"):\n",
    "                    clean_link = clean_link[:-1]\n",
    "                clean_links.append(clean_link)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = {url}\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "        try:\n",
    "            # Save text from the url to a <url>.txt file\n",
    "            with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\") as f:\n",
    "    \n",
    "                # Get the text from the URL using BeautifulSoup\n",
    "                soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "    \n",
    "                # Get the text but remove the tags\n",
    "                text = soup.get_text()\n",
    "    \n",
    "                # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "                if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                    print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "                \n",
    "                # Otherwise, write the text to the file in the text directory\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(e)  \n",
    "        \n",
    "        try:    \n",
    "            # Get the hyperlinks from the URL and add them to the queue\n",
    "            for link in get_domain_hyperlinks(local_domain, url):\n",
    "                try:\n",
    "                    if link not in seen:\n",
    "                        queue.append(link)\n",
    "                        seen.add(link)\n",
    "                except Exception as e:\n",
    "                    print(e)          \n",
    "        except Exception as e:\n",
    "            print(e)            \n",
    "\n",
    "crawl(full_url)"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ],
   "id": "40663cb7d6e4ef76",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "domain_dir = os.path.join(\"text\", domain)\n",
    "abs_domain_dir = os.path.abspath(domain_dir)\n",
    "\n",
    "if os.path.exists(abs_domain_dir):\n",
    "    # Get all the text files in the text directory\n",
    "    for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "        \n",
    "        try:\n",
    "            # Open the file and read the text\n",
    "            with open(\"text/\" + domain + \"/\" + file, \"r\") as f:\n",
    "                text = f.read()\n",
    "        \n",
    "                # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "                texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred during reading file '{text}': {e}\")       \n",
    "else:\n",
    "    print(f\"Directory '{abs_domain_dir}' does not exist.\")\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ],
   "id": "5bdf6cdd793a807f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "# Tokenize the text and save the number of tokens to a new column\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "df.n_tokens.hist()"
   ],
   "id": "915ffecc9c9eb887",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_tokens = 500\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    # Add the last chunk to the list of chunks\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "shortened = []\n",
    "\n",
    "# Loop through the dataframe\n",
    "for row in df.iterrows():\n",
    "\n",
    "    # If the text is None, go to the next row\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    # Otherwise, add the text to the list of shortened texts\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ],
   "id": "d798a3797291d28e",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "df.n_tokens.hist()"
   ],
   "id": "aeb1a0aeda2534ac",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import openai\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key=api_key\n",
    "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
    "df.to_csv('processed/embeddings.csv')\n",
    "df.head()"
   ],
   "id": "2fbb06d162be93d0",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "from openai.embeddings_utils import distances_from_embeddings, cosine_similarity\n",
    "\n",
    "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(literal_eval).apply(np.array)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# SAVE\n",
    "with open('df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n"
   ],
   "id": "eed7887720ceb9b5",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T23:12:41.669280Z",
     "start_time": "2024-06-21T23:12:41.663711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_random_port(min_port=1024, max_port=49151):\n",
    "    while True:\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                port = random.randint(min_port, max_port)\n",
    "                s.bind(('', port))\n",
    "                _, bound_port = s.getsockname()\n",
    "                if bound_port == port:\n",
    "                    return port\n",
    "        except OSError:\n",
    "            continue"
   ],
   "id": "4c40ad2421010ee2",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T23:14:31.112763Z",
     "start_time": "2024-06-21T23:14:31.098230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import streamlit as st\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import random\n",
    "import socket\n",
    "\n",
    "streamlit_app = \"\"\"\n",
    "import streamlit as st\n",
    "\n",
    "def main():\n",
    "    st.title(\"Simple Streamlit App\")\n",
    "\n",
    "    # Display a text input box\n",
    "    user_input = st.text_input(\"Enter something:\")\n",
    "\n",
    "    # Display a button\n",
    "    if st.button(\"Submit\"):\n",
    "        # Display the input text\n",
    "        st.write(\"You entered:\", user_input)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Temporary file to hold the Streamlit app\n",
    "with tempfile.NamedTemporaryFile('w', delete=False, suffix='.py') as temp_file:\n",
    "    temp_file.write(streamlit_app)\n",
    "    temp_file_name = temp_file.name\n",
    "\n",
    "random_port = get_random_port()\n",
    "\n",
    "# Define the Streamlit command\n",
    "streamlit_command = f\"streamlit run {temp_file_name} --server.port {random_port} --server.address 0.0.0.0\"\n",
    "\n",
    "# Print the URL for the user to visit\n",
    "print(f\"Visit the Streamlit app at http://localhost:{random_port}\")\n",
    "\n",
    "# Use subprocess to run the Streamlit command\n",
    "process = subprocess.Popen(streamlit_command, shell=True)\n",
    "\n"
   ],
   "id": "4e87981a1526d23d",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T23:58:07.962302Z",
     "start_time": "2024-06-21T23:33:18.318701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import openai\n",
    "from openai.embeddings_utils import distances_from_embeddings\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "random_port = get_random_port()\n",
    "print(f\"Random port: {random_port}\")\n",
    "\n",
    "streamlit_command = f\"streamlit run web_page.py --server.port {random_port} --server.address 0.0.0.0\"\n",
    "print(streamlit_command)\n",
    "print(f\"Visit the site at http://localhost:{random_port}\")\n",
    "\n",
    "try:\n",
    "    # Start the Streamlit app\n",
    "    process = subprocess.Popen(streamlit_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output and error streams\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        error = process.stderr.readline()\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        if error:\n",
    "            print(error.strip())\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ],
   "id": "91e42ac34603074f",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T23:30:47.535306Z",
     "start_time": "2024-06-21T23:30:47.504389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=f\"http://localhost:{random_port}\", width=1000, height=600)\n"
   ],
   "id": "bc31fd75a476b181",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "a08a7bc2c6cb4676",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
